{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "363fc317-5022-4167-b1ae-985d88bd9df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7b320da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0bb8505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lshre\\Gen-AI\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68189547-f601-4298-b286-f926e5851cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dab8aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1c20695-9bbc-48a4-a271-e1191406f3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_path = (r'C:\\Users\\lshre\\Gen-AI\\Data_Preprocessing\\Data\\IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34b07047-bf72-4315-ae9a-b4f906e1b1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(Data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0fc840a-b653-47b3-baaa-8c0ff1b0d3c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f63cfa6-eaa9-4cef-9b45-89637c8ce3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               review sentiment\n",
      "0   One of the other reviewers has mentioned that ...  positive\n",
      "1   A wonderful little production. <br /><br />The...  positive\n",
      "2   I thought this was a wonderful way to spend ti...  positive\n",
      "3   Basically there's a family where a little boy ...  negative\n",
      "4   Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "..                                                ...       ...\n",
      "95  Daniel Day-Lewis is the most versatile actor a...  positive\n",
      "96  My guess would be this was originally going to...  negative\n",
      "97  Well, I like to watch bad horror B-Movies, cau...  negative\n",
      "98  This IS the worst movie I have ever seen, as w...  negative\n",
      "99  I have been a Mario fan for as long as I can r...  positive\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data.head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf203257",
   "metadata": {},
   "source": [
    "**LowerCase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d00cd92d-db33-4db6-b9ac-5646b1705893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['review'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d7fbde9-e9a6-4224-88aa-5a4e8f9af698",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['review'] = data['review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d6191ac-b577-472a-ad9c-849844c7075f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        one of the other reviewers has mentioned that ...\n",
      "1        a wonderful little production. <br /><br />the...\n",
      "2        i thought this was a wonderful way to spend ti...\n",
      "3        basically there's a family where a little boy ...\n",
      "4        petter mattei's \"love in the time of money\" is...\n",
      "                               ...                        \n",
      "49995    i thought this movie did a down right good job...\n",
      "49996    bad plot, bad dialogue, bad acting, idiotic di...\n",
      "49997    i am a catholic taught in parochial elementary...\n",
      "49998    i'm going to have to disagree with the previou...\n",
      "49999    no one expects the star trek movies to be high...\n",
      "Name: review, Length: 50000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88335536",
   "metadata": {},
   "source": [
    "**Removing HTML Tag**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6eca2b54-8fd6-4bc8-80a7-c5b056bda0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_html_tag(text):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub(r'', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0d11888",
   "metadata": {},
   "outputs": [],
   "source": [
    "text =  \"<html><body><p> Movie 1</p><p> Actor - Aamir Khan</p><p> Click here to <a href='http://google.com'>download</a></p></body></html>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d229af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Movie 1 Actor - Aamir Khan Click here to download'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_html_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f817fb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        one of the other reviewers has mentioned that ...\n",
      "1        a wonderful little production. the filming tec...\n",
      "2        i thought this was a wonderful way to spend ti...\n",
      "3        basically there's a family where a little boy ...\n",
      "4        petter mattei's \"love in the time of money\" is...\n",
      "                               ...                        \n",
      "49995    i thought this movie did a down right good job...\n",
      "49996    bad plot, bad dialogue, bad acting, idiotic di...\n",
      "49997    i am a catholic taught in parochial elementary...\n",
      "49998    i'm going to have to disagree with the previou...\n",
      "49999    no one expects the star trek movies to be high...\n",
      "Name: review, Length: 50000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data['review'] = data['review'].apply(remove_html_tag)\n",
    "\n",
    "print(data['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9e279a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no_tag_list = data['review'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfac9d0",
   "metadata": {},
   "source": [
    "URL Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62050355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "727503dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['Check out my youtube https://www.youtube.com/dswithbappy dswithbappy',\n",
    "'Check out my linkedin https://www.linkedin.com/in/boktiarahmed73/',\n",
    "'Google search here www.google.com',\n",
    "'For data click https://www.kaggle.com/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0976f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Check out my youtube  dswithbappy', 'Check out my linkedin ', 'Google search here ', 'For data click ']\n"
     ]
    }
   ],
   "source": [
    "cleaned_text = [remove_url(text) for text in texts]\n",
    "\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3357622f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        one of the other reviewers has mentioned that ...\n",
      "1        a wonderful little production. the filming tec...\n",
      "2        i thought this was a wonderful way to spend ti...\n",
      "3        basically there's a family where a little boy ...\n",
      "4        petter mattei's \"love in the time of money\" is...\n",
      "                               ...                        \n",
      "49995    i thought this movie did a down right good job...\n",
      "49996    bad plot, bad dialogue, bad acting, idiotic di...\n",
      "49997    i am a catholic taught in parochial elementary...\n",
      "49998    i'm going to have to disagree with the previou...\n",
      "49999    no one expects the star trek movies to be high...\n",
      "Name: review, Length: 50000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data['review'] = data['review'].apply(remove_url)\n",
    "\n",
    "print(data['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062c6dd4",
   "metadata": {},
   "source": [
    "**Punctuation Handling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83e52ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "exclude = string.punctuation\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    punctuationless = text.translate(str.maketrans('', '', exclude))\n",
    "\n",
    "    return punctuationless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3c4ea65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        one of the other reviewers has mentioned that ...\n",
      "1        a wonderful little production the filming tech...\n",
      "2        i thought this was a wonderful way to spend ti...\n",
      "3        basically theres a family where a little boy j...\n",
      "4        petter matteis love in the time of money is a ...\n",
      "                               ...                        \n",
      "49995    i thought this movie did a down right good job...\n",
      "49996    bad plot bad dialogue bad acting idiotic direc...\n",
      "49997    i am a catholic taught in parochial elementary...\n",
      "49998    im going to have to disagree with the previous...\n",
      "49999    no one expects the star trek movies to be high...\n",
      "Name: review, Length: 50000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data['review'] = data['review'].apply(remove_punctuation)\n",
    "\n",
    "print(data['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e237e8d2",
   "metadata": {},
   "source": [
    "**Chat_Conversion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "637d2059",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words = {\n",
    "    'AFAIK':'As Far As I Know',\n",
    "    'AFK':'Away From Keyboard',\n",
    "    'ASAP':'As Soon As Possible',\n",
    "    \"FYI\": \"For Your Information\",\n",
    "    \"ASAP\": \"As Soon As Possible\",\n",
    "    \"BRB\": \"Be Right Back\",\n",
    "    \"BTW\": \"By The Way\",\n",
    "    \"OMG\": \"Oh My God\",\n",
    "    \"IMO\": \"In My Opinion\",\n",
    "    \"LOL\": \"Laugh Out Loud\",\n",
    "    \"TTYL\": \"Talk To You Later\",\n",
    "    \"GTG\": \"Got To Go\",\n",
    "    \"TTYT\": \"Talk To You Tomorrow\",\n",
    "    \"IDK\": \"I Don't Know\",\n",
    "    \"TMI\": \"Too Much Information\",\n",
    "    \"IMHO\": \"In My Humble Opinion\",\n",
    "    \"ICYMI\": \"In Case You Missed It\",\n",
    "    \"AFAIK\": \"As Far As I Know\",\n",
    "    \"BTW\": \"By The Way\",\n",
    "    \"FAQ\": \"Frequently Asked Questions\",\n",
    "    \"TGIF\": \"Thank God It's Friday\",\n",
    "    \"FYA\": \"For Your Action\",\n",
    "    \"ICYMI\": \"In Case You Missed It\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0bc2b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_words_conversion(text):\n",
    "    new_text = []\n",
    "    # Split the input text into words\n",
    "    for word in str(text).split():\n",
    "        if word.upper() in chat_words:\n",
    "            new_text.append(chat_words[word.upper()])\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    # Join the words back into a sentence\n",
    "    return ' '.join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5bcbdfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = 'lol this is a good movie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b5c56b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laugh Out Loud this is a good movie\n"
     ]
    }
   ],
   "source": [
    "new_dummy = chat_words_conversion(dummy)\n",
    "print(new_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cce5a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        one of the other reviewers has mentioned that ...\n",
      "1        a wonderful little production the filming tech...\n",
      "2        i thought this was a wonderful way to spend ti...\n",
      "3        basically theres a family where a little boy j...\n",
      "4        petter matteis love in the time of money is a ...\n",
      "                               ...                        \n",
      "49995    i thought this movie did a down right good job...\n",
      "49996    bad plot bad dialogue bad acting idiotic direc...\n",
      "49997    i am a catholic taught in parochial elementary...\n",
      "49998    im going to have to disagree with the previous...\n",
      "49999    no one expects the star trek movies to be high...\n",
      "Name: review, Length: 50000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to the pandas Series\n",
    "data['review'] = data['review'].apply(chat_words_conversion)\n",
    "\n",
    "print(data['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd591bf0",
   "metadata": {},
   "source": [
    "**Incorrect Text Handling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f333474",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def correct_spellings(text):\n",
    "        b = TextBlob(text) # 'b' is the object for the TextBlob class\n",
    "\n",
    "        return b.correct().string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dc0fdc38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have good spelling'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dummy = 'I havv gooode speling'\n",
    "\n",
    "correct_spellings(Dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "387b182f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 100/100 [05:46<00:00,  3.46s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Dummy = pd.DataFrame()  # Ensure Dummy is a DataFrame\n",
    "Dummy['comment'] = data['review'].head(100).swifter.apply(correct_spellings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "69798392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              comment\n",
      "0   one of the other reviews has mentioned that af...\n",
      "1   a wonderful little production the filling tech...\n",
      "2   i thought this was a wonderful way to spend ti...\n",
      "3   basically there a family where a little boy ja...\n",
      "4   letter matters love in the time of money is a ...\n",
      "..                                                ...\n",
      "95  daniel daylewis is the most versatile actor al...\n",
      "96  my guess would be this was originally going to...\n",
      "97  well i like to watch bad horror bodies cause i...\n",
      "98  this is the worst movie i have ever seen as we...\n",
      "99  i have been a marie fan for as long as i can r...\n",
      "\n",
      "[100 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(Dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e648a0b3",
   "metadata": {},
   "source": [
    "**Stop_Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fed56ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lshre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab4fcf42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "81e78509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text = []\n",
    "\n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    x = new_text[:]\n",
    "    new_text.clear()\n",
    "    return \" \".join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f05f0231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probably  all-time favorite movie,  story  selflessness, sacrifice  dedication   noble cause,    preachy  boring.   never gets old, despite   seen   15   times'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords('probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it\\'s not preachy or boring. it just never gets old, despite my having seen it some 15 or more times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "015e3bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 100/100 [00:07<00:00, 12.71it/s]\n"
     ]
    }
   ],
   "source": [
    "Dummy['comment'] = Dummy['comment'].swifter.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a9906d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              comment\n",
      "0   one    reviews  mentioned   watching  1 oz epi...\n",
      "1    wonderful little production  filling techniqu...\n",
      "2    thought    wonderful way  spend time    hot s...\n",
      "3   basically   family   little boy jake thinks   ...\n",
      "4   letter matters love   time  money   usually st...\n",
      "..                                                ...\n",
      "95  daniel daylewis    versatile actor alive engli...\n",
      "96   guess would    originally going    least two ...\n",
      "97  well  like  watch bad horror bodies cause  thi...\n",
      "98     worst movie   ever seen  well   worst    pr...\n",
      "99      marie fan   long    remember    fond memor...\n",
      "\n",
      "[100 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(Dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b18a8c1",
   "metadata": {},
   "source": [
    "**Remove_Emojies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e69c89b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fdc0aa75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loved the movie. It was '"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emoji(\"Loved the movie. It was 😘😘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6659f38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is :fire:\n",
      "Loved the movie. It was :face_blowing_a_kiss:\n"
     ]
    }
   ],
   "source": [
    "print(emoji.demojize('Python is 🔥'))\n",
    "\n",
    "print(emoji.demojize('Loved the movie. It was 😘'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "af570c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def easy_remove_emoji(text):\n",
    "    return emoji.demojize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5c2d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 100/100 [00:00<00:00, 621.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              comment\n",
      "0   one    reviews  mentioned   watching  1 oz epi...\n",
      "1    wonderful little production  filling techniqu...\n",
      "2    thought    wonderful way  spend time    hot s...\n",
      "3   basically   family   little boy jake thinks   ...\n",
      "4   letter matters love   time  money   usually st...\n",
      "..                                                ...\n",
      "95  daniel daylewis    versatile actor alive engli...\n",
      "96   guess would    originally going    least two ...\n",
      "97  well  like  watch bad horror bodies cause  thi...\n",
      "98     worst movie   ever seen  well   worst    pr...\n",
      "99      marie fan   long    remember    fond memor...\n",
      "\n",
      "[100 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Dummy['comment'] = Dummy['comment'].swifter.apply(easy_remove_emoji)\n",
    "\n",
    "print(Dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078b534e",
   "metadata": {},
   "source": [
    "**Tokenization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0b20ae",
   "metadata": {},
   "source": [
    "**1) Using Split Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1660a521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'going', 'to', 'delhi']\n",
      " It doesn't split the character \"!\" in  ['I', 'am', 'going', 'to', 'delhi!']\n"
     ]
    }
   ],
   "source": [
    "# word tokenization\n",
    "sent1 = 'I am going to delhi'\n",
    "print(sent1.split())\n",
    "\n",
    "# Problems with split function\n",
    "sent3 = 'I am going to delhi!'\n",
    "print(' It doesn\\'t split the character \"!\" in ', sent3.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "370fc599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am going to delhi',\n",
       " ' I will stay there for 3 days',\n",
       " \" Let's hope the trip to be great\"]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence tokenization\n",
    "sent2 = 'I am going to delhi. I will stay there for 3 days. Let\\'s hope the trip to be great'\n",
    "sent2.split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd709cd0",
   "metadata": {},
   "source": [
    "**Regular Expression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b636b7ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word Tokenization\n",
    "import re\n",
    "sent3 = 'I am going to delhi!'\n",
    "tokens = re.findall(r\"[\\w']+\", sent3)#\\w is equivalent to [a-zA-Z0-9_] and r is used to convert the normal string to raw string\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c37638eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Lorem Ipsum is simply dummy text of the printing and typesetting industry?\\nLorem Ipsum has been the industry's standard dummy text ever since the 1500s,\\nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3#Sentence Tokenization\n",
    "text = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry?\n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\n",
    "when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\n",
    "sentences = re.compile('[.!?] ').split(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe5bf5b",
   "metadata": {},
   "source": [
    "**NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef711e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lshre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8868b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\lshre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')# Download the Punkt tokenizer if needed\n",
    "nltk.download('punkt_tab')# Download the Punkt tokenizer if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ae3652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'visit', 'delhi', '!']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1 = 'I am going to visit delhi!'# Word Tokenization\n",
    "word_tokenize(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d0260575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem Ipsum is simply dummy text of the printing and typesetting industry?',\n",
       " \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\\nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry?\n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\n",
    "when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"# Sentence Tokenization\n",
    "\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aa4dbc",
   "metadata": {},
   "source": [
    "**Spacy (Good) - Another way to tokenize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e88c0945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "51da7204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(I am going to delhi!,\n",
       " I am going to delhi. I will stay there for 3 days. Let's hope the trip to be great,\n",
       " I am going to visit delhi!)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2 = nlp(sent3)\n",
    "doc3 = nlp(sent2)\n",
    "doc4 = nlp(sent1)\n",
    "\n",
    "doc2, doc3, doc4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e325d1f",
   "metadata": {},
   "source": [
    "**Stemmer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0a74744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "90a4d3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aa7b5812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat eat eat'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = \"eat eating eats\"\n",
    "stem_words(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6fa95895",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 100/100 [00:00<00:00, 273.41it/s]\n"
     ]
    }
   ],
   "source": [
    "Dummy['comment'] = Dummy['comment'].swifter.apply(stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e24c9b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              comment\n",
      "0   one review mention watch 1 oz episod hook righ...\n",
      "1   wonder littl product fill techniqu assum oldti...\n",
      "2   thought wonder way spend time hot summer weeke...\n",
      "3   basic famili littl boy jake think combin close...\n",
      "4   letter matter love time money usual stun film ...\n",
      "..                                                ...\n",
      "95  daniel daylewi versatil actor aliv english ari...\n",
      "96  guess would origin go least two part thu least...\n",
      "97  well like watch bad horror bodi caus think int...\n",
      "98  worst movi ever seen well worst probabl ever s...\n",
      "99  mari fan long rememb fond memori play super ma...\n",
      "\n",
      "[100 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(Dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9933751",
   "metadata": {},
   "source": [
    "**Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "86d5c4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lshre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\lshre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8674ba11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 have                \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word,pos='v')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683ac65b",
   "metadata": {},
   "source": [
    "NOTE: Stemming & lamatization are same to retrieve root words but lamatization is worked good. Lamatization is slow & stemming is fast"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
